---
layout: post
title: Weekly Report - 2018-06-25 to 2018-06-29
---

### Introduction: about stochastic gradient descent

Since computing the probability density function that maximizes entropy for a given
set of presence-only data is equivalent to maximizing a [convex function](https://en.wikipedia.org/wiki/Convex_function), a maximum is guaranteed
to exist at all times. However, the speed of convergence and the quality of the
approximation depends (ideally) on randomly resampling the space at many locations
on the first iterations, and later on a 'telescopic' descent into the regions of the
space where the solution is present.

The [code](http://scikit-learn.org/stable/modules/sgd.html) used to compute the
stochastic gradient descent at present is provided by the scikit-learn package.
However, there are efficiency penalties and API mismatches related to our representation
that still need solution.

### Present state of the code

This week's work concentrated most of the effort in completing the code for having
a fully functional version of intros-MaxEnt. The state machine was refactored, as
well as the representation of the grid in order to make it compatible with the
stochastic gradient descent solver.

### Next steps

For Week 7, the main goal is to build and upload a first complete package version
to PyPI, as well as working on mapping and datasets. At present, integration issues
remain with existing stochastic gradient descent algorithms and our implementation
of MaxEnt.
