---
layout: post
title: Weekly Report - 2018-06-04 to 2018-06-08
---

### Introduction: integrating the algorithmics into a usable model

This week the effort has been focused on bringing together parts of the implementation into a model structure. There are several goals with respect to users, inspired by various packages (including [OpenRefine](http://openrefine.org)). At least, we want the package to work both within and outside of The Whole Tale. The latter, for instance will ensure that people can easily preserve one Jupyter Notebook in the WT environment while playing with parts of the model.

The following list of user requirements comprises the current effort:

- The model should be able to capture the notion of an experiment. For reproducibility to occur, the practices of computational science must change. Instead of 'running code', we must move towards executing an experiment.
- Users should be able to annotate their experiments. The bioinformatics community has already solved many provenance issues related to giving credit to scholars. This is essential also to increase recognition of scholars who must also become their own [Research Software Engineers](http://rse.ac.uk/who/).
- All the context of an experiment should be loadable from a file, including all required elements to initialize the abstract machine.
- The whole experiment should be 'packageable' with all the necessary elements to replicate and inspect it.
- If a presence-only dataset does not have geolocated points and only some form of approximate landmark, users should be capable of providing the matching fields with centroids.
- All aspects of the model should be configurable. Specially those that provide the contexts.
- All operations should be logged in order to help users save their line of reasnoning, specially when using a Jupyter Notebook.
- All operations should yield errors and warnings when appropriate in relation to the software and the potential theoretical mishaps.
- The model should keep track of what is valid with respect to the stage of its development. We want scholars to get the feeling that they carve their experiments out of data.
- The model should be visualizable in PNG, KML and CVS format using WGS85 (ESRI:4326)

### Handling geographic position data

The code supports now two use cases. Suppose we have a finite list of PO data with state and county as defined by the GBIF standard. If users want to manually map the data in a list, they only need to provide pairs of *(state, county)*. For instance (based on the Weakley 2015 dataset):

```python
gecoder = gc.GeoCode('counties_curated.csv')
ctylist = [('Alabama', 'Lee'),
           ('Alabama', 'Macon'),
           ('Louisiana', 'Calcasieu'),
           ('Louisiana', 'Plaquemines'),
           ('Florida', 'Charlotte'),
           ('Florida', 'Escambia'),
           ('Florida', 'Palm Beach'),
           ]
outcome = gecoder.georeflist(['stateProvince','county','decimalLatitude','decimalLongitude'], ctylist)
print outcome
```

which uses the geographical reference file *counties_curated* to produce

```
     stateProvince       ...        decimalLongitude
1145       Alabama       ...              -85.355612
1148       Alabama       ...              -85.692607
2303     Louisiana       ...              -93.357948
2329     Louisiana       ...              -89.654993
2351       Florida       ...              -82.108088
2335       Florida       ...              -87.377680
1994       Florida       ...              -80.432910
```

The second use case study is when PO data is stored in a CSV file. In this case, the file is used along with the landmark keys (column headers) for the 

### Provenance information for environmental features

One of the central elements of ecological niche models is choosing the right features that may predict an organism's behavior. We have a new capability that documents features expected to be found in the model. Suppose we have the file 'features.csv' for a three-fingered sloth:

```csv
Feature,Description,DOI
TEMP,"Temperature","10.2307/1374287"
HUM,"Relative humidity","10.1007/BF00694571"
```

With our framework, it is possible to use a FeaturesReader object based on this file, consume the data and ask for the complete description:

```python
fs = pv.FeaturesReader('features.csv')
fs.consume()
fs.describe()
```

The description operation yields the outcome below.

```
Model Features
================================

  Feature:TEMP
  Description:Temperature
  Reference: Britton, S. W., & Atkinson, W. E. (1938). Poikilothermism in the Sloth. Journal of Mammalogy, 19(1), 94. doi:10.2307/1374287

  Feature:HUM
  Description:Relative humidity
  Reference: Dawson, T. J., & Degabriele, R. (1973). The cuscus (Phalanger maculatus)â¯? a marsupial sloth? Journal of Comparative Physiology, 83(1), 41â50. doi:10.1007/bf00694571
```

### Next steps

At present, the pending task is to finalize the integration of the model into the final package. Part of the delay has been due to testing both implementations of MaxEnt and discovering that probability leaks in the original one.

Thanks to Nico Frank's guidance, we will focus on building a species plan for weeks 5-10 using various plant species and observation. In particular, we will build tests that for the same species, geographic regions and environmental conditions differentially compare between years of presence only data.

